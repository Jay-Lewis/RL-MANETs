{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Problem Summary:\n",
    "\n",
    "* _Overall goal:_ apply Reinforcement Learning to packet route optimization for Wireless Ad Hoc Networks (WANETs) and Mobilized Ad Hoc Networks (MANETs)\n",
    "\n",
    "\n",
    "* Route optimization for WANETs is difficult\n",
    "    * network topology is dynamic\n",
    "    * generally decentralized\n",
    "    \n",
    "\n",
    "* Additional problem difficulties:\n",
    "    * transmission cost / interference\n",
    "    * power constraints\n",
    "    * scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANETs:\n",
    "\n",
    "* decentralized network structures\n",
    "\n",
    "\n",
    "* nodes are mobile:\n",
    "    * can move in space\n",
    "    * may join/leave any time\n",
    "    \n",
    "    \n",
    "* _applications_:\n",
    "    * mobile internet (e.g: project LOON)\n",
    "    * mobile phone networks / SPANs\n",
    "    * sensor networks\n",
    "    * vehicular networks / VANETs\n",
    "    * smart cities\n",
    "    * military applications\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "<img align=\"left\" src=\"photos/WANET2.png\">\n",
    "\n",
    "_Range of communication demonstrated by surrounding circle_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication Scheme:\n",
    "\n",
    "Each device uses radio waves to communicate packets to other devices. The broadcasted message is transmitted between nodes over a certain frequency channel. If two devices use the channel simulatenously, a \"collision\" occurs. This nullifies both broadcasts. \n",
    "<img src=\"photos/interference.png\">\n",
    "   \n",
    "This can cause complex interactions amongst the communication nodes. For example in the _\"hidden node problem\"_ (see below), node $N_{A}$ can see $N_{B}$, but cannot see $N_{C}$. If $N_{A}$ and $N_{C}$ both communicate to $N_{B}$ simultaneously, then a collision will occur; furthermore, node $N_{A}$ cannot attribute this collision to $N_{C}$ unless information is shared amongst the nodes. \n",
    "\n",
    "\n",
    "![alt text](https://www.cse.iitk.ac.in/users/dheeraj/cs425/fig.lec05/image002.gif)\n",
    "\n",
    "These interference problems coupled with a decentralized control scheme can lead to inefficient routing. A large portion of time can be spent rebroadcasting messages which previously collided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Solutions (abridged):\n",
    "\n",
    "1. Reactive routing (AODV):\n",
    "\n",
    "    * This method is reactive because nodes do not save information regarding the entire network topology. They only maintain connections to local neighbors. \n",
    "\n",
    "    * Nodes periodically send \"hello\" messages to gain information about their immediate neighbors. \n",
    "\n",
    "    * Whenever a node wishes to send a packet through the network, it sends a \"route request\" (RREQ) to find a path to the destination. This request is passed along the network following a fixed protocol until the destination is found. Then the information is passed back through the network to the sending node. \n",
    "<img src=\"photos/AODV.png\">\n",
    "\n",
    "2. Proactive routing (OLSR):\n",
    "\n",
    "    * This method is proactive as the nodes maintain a local \"image\" of the network's full topology. \n",
    "\n",
    "    * Nodes periodically send \"hello\" messages to gain information about their immediate neighbors. \n",
    "\n",
    "    * Additionally, nodes broadcast their local connections to the rest of the network through \"flooding\". Essentially, the information is spread to every connected node in the network by a fixed policy. \"Flooding\" can cause unnecessary transmission and cause packet collisions.\n",
    "\n",
    "    * With a local image of the network, a version of shortest path algorithm is applied to find a suitable path to the destination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Reinforcement Learning?:\n",
    "\n",
    "*  More direct, global combinatiorial optimization in a distributed fashion seems unlikely to scale effectively (finding the exact optimum is NP-hard I would imagine)\n",
    "* The \"best\" strategy given limited computation time and limited power usage is difficult to derive directly \n",
    "* RL agents can learn how to best encorporate available information at a node to improve routing (while maintaining constraints such as power usage)\n",
    "* RL agents can learn how to most effectively cooperate (direct analysis is difficult). Ex: how often and how much information should be shared amongst neighbors.\n",
    "* The computation time complexity of a DRL solution could potentially be smaller (i.e. faster) than a more straightforward optimization algorithm (latency is important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Problem Constraints:\n",
    "\n",
    "1. Decentralized control\n",
    "\n",
    "2. Multi-agent learning scheme\n",
    "\n",
    "3. Full cooperation (protocol enforced)\n",
    "\n",
    "4. Arbitrary intercommunication (with associated cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical / Environmental Constraints\n",
    "\n",
    "1. Number of unique frequency channels $N_{c}$ is variable and will be analyzed to see achievability of specific data rates\n",
    "2. Range of unique frequency channels is proportional to transmission frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agent State Space:\n",
    "\n",
    "* there are a number of potential state types which may prove useful in deriving the best policy. Below is a list of state classes and specifics.\n",
    "\n",
    "\n",
    "1. Routing Information:\n",
    "    * Packet source / destination\n",
    "\n",
    "2. Environment Information:\n",
    "    * Current estimate of topology of network\n",
    "    * Strength of network connections\n",
    "    * Physical location and heading of nodes (possible?)\n",
    "\n",
    "3. System Information:\n",
    "    * number packets in local buffer\n",
    "    * battery state\n",
    "    \n",
    "4. History\n",
    "    * time of day\n",
    "    * time elapsed between certain actions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agent Action Space:\n",
    "\n",
    "* similar to the state space, there are a number of distinct actions which can be considered.\n",
    "* I would imagine that the action space should be structured as a sequence of decisions within time slots (viewed as a time series)\n",
    "\n",
    "\n",
    "1. Packet Related\n",
    "\n",
    "    * Forward packet to neighbor (on specific channel)\n",
    "    * Drop packet\n",
    "    * Save packet to buffer\n",
    "    * WAIT (do nothing)\n",
    "\n",
    "2. Environment related\n",
    "\n",
    "    * Sense channels (local connections)\n",
    "    * Measure local node locations (may not be physically feasable)\n",
    "    \n",
    "3. Cooperation related\n",
    "\n",
    "    * Proactively / reactively share information with nodes in network\n",
    "    * Proactively / reactively request information from network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Signal / Reinforcement Protocol\n",
    "\n",
    "* the reinforcement signal should be related to the following:\n",
    "\n",
    "    * Quality of service (QoS)\n",
    "    * Transmission slowdown / delay\n",
    "    * Power usage\n",
    "    * Transmission collisions\n",
    "    \n",
    "* How the reinforcement signal should be exactly formulated will require more considereation.\n",
    "* The way in which the reinforcement signal is provided to an agent is also an open question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Thoughts:\n",
    "\n",
    "* nodes can \"sniff\" packets and infer the graph structure\n",
    "* nodes could proactively probe the network to learn its structure (causal graph inference)\n",
    "* sharing information with neighboring nodes comes at a cost \n",
    "    * how can the agent's local information be shared at different sizes?\n",
    "    * essentially a variable-sized embedding?\n",
    "* perhaps it is wise to share some information at times which are less busy? (offline learning)\n",
    "* it seems that the model complexity of each node in the network could be variable and dependent on its position in the network\n",
    "    * less connected nodes probably are less important in the global optimization and thus require less information\n",
    "    * spend power most efficintly based on network position\n",
    "* if a core model is shared amongst the agents, then it may improve coordination\n",
    "    * each node can anticipate the actions of the others\n",
    "    * maybe some portion of the model is shared and another is not (role assignment)\n",
    "* what about fairness? \n",
    "    * for example, in a mobile phone network the users closest to the bay stations will receive higher traffic loads\n",
    "    * perhaps these methods make more sense in settings where fairness isn't as relevant (team strategy vs. mutual cooperation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
